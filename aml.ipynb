# <div align = "center"> Cassava Leaf Image Analysis for Disease Identification and Classification </div>

### Problem Statement
To build an image classifier using a deep learning algorithm to classify cassava images into four disease categories or a fifth category indicating a healthy leaf. With this, farmers will be able to quickly identify diseased plants by taking images of their crops with mobile-quality cameras to quickly identify diseased plants, potentially saving their crops before they inflict irreparable damage.
import os
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="0"
import numpy as np
import pandas as pd
import cv2
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from sklearn.utils import class_weight

import json
import math
from datetime import datetime

from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import LabelBinarizer
from sklearn.model_selection import train_test_split, StratifiedKFold

import tensorflow as tf
# import tensorflow.compat.v1 as tf
# tf.disable_v2_behavior()
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers, models, callbacks, optimizers, applications, experimental, losses, utils 

# import keras
# from keras.layers import Input#, Dropout, Dense, MaxPooling2D, ZeroPadding2D, Conv2D, Flatten

import warnings
warnings.simplefilter("ignore")
os.listdir('cassava-leaf-disease-classification')
print("There are", len(os.listdir('cassava-leaf-disease-classification/train_images')), "images in this dataset")
with open('cassava-leaf-disease-classification/label_num_to_disease_map.json') as file:
    disease_class = json.loads(file.read())
    print(json.dumps(disease_class, indent=2))
df_train = pd.read_csv('cassava-leaf-disease-classification/train.csv')
df_train.head()
# Including the class names in the data
df_train['class'] = df_train['label'].map({int(i) : c for i, c in disease_class.items()}) 
df_train.head()
df_train['class'].value_counts().to_frame().reset_index()
plt.figure(figsize=(10, 6))

sns.barplot(x='class', y='index', data=df_train['class'].value_counts().to_frame().reset_index())
plt.xlabel('Count')
plt.ylabel('Classes')
plt.show()
np.round((df_train['class'].value_counts() / len(df_train['class'])) * 100, 2)
We see that there is a heavy class imbalance in the dataset with class 3 (Cassava Mosaic Disease (CMD)) have]ing 61% of the dataset while class 0  (Cassava Bacterial Blight (CBB)) has only 5% of the dataset.

We should explore ways to balance this dataset
## Exploratory Data Analysis
path = "cassava-leaf-disease-classification"
def plot(images, labels, predictions = None):
    n_cols = min(3, len(images))
    n_rows = math.ceil(len(images) / n_cols)
    fig, axes = plt.subplots(n_rows, n_cols, figsize = (21,16))

    if predictions is None:
              
        predictions = [None] * len(labels)


    for i, (x, y_true, y_pred) in enumerate(zip(images, labels, predictions)):
        
        ax = axes.flat[i]
        a = plt.imread(os.path.join(path,"train_images", x ))
        ax.imshow(a)

        
        ax.set_title(f"Class: {y_true}")
             
        if y_pred is not None:
            ax.set_xlabel(f"Pred: {y_pred}", color='blue', fontweight='bold')

        ax.set_xticks([])
        ax.set_yticks([])
### Samples of "0": "Cassava Bacterial Blight (CBB)"
df_0 = df_train[df_train['label']==0]
df_0 = df_0.sample(9)
df_0_id = df_0['image_id'].values
df_0_class = df_0['class'].values

plot(df_0_id, df_0_class)
### Samples of "1": "Cassava Brown Streak Disease (CBSD)"
df_1 = df_train[df_train['label']==1]
df_1 = df_1.sample(12)
df_1_id = df_1['image_id'].values
df_1_class = df_1['class'].values

plot(df_1_id, df_1_class)
### Samples of "2": "Cassava Green Mottle (CGM)"
df_2 = df_train[df_train['label']==2]
df_2 = df_2.sample(12)
df_2_id = df_2['image_id'].values
df_2_class = df_2['class'].values

plot(df_2_id, df_2_class)
### Samples of   "3": "Cassava Mosaic Disease (CMD)"
df_3 = df_train[df_train['label']==3]
df_3 = df_3.sample(12)
df_3_id = df_3['image_id'].values
df_3_class = df_3['class'].values

plot(df_3_id, df_3_class)
### Samples of "4": "Healthy"
df_4 = df_train[df_train['label']==4]
df_4 = df_4.sample(12)
df_4_id = df_4['image_id'].values
df_4_class = df_4['class'].values

plot(df_4_id, df_4_class)
## Baseline Accuracy with Data Imbalance
y_pred = [3] * len(df_train.label)
print("The baseline accuracy is {}".format(accuracy_score(y_pred, df_train.label)))
The default accuracy is 61% since label "3" represents 61% of the entire data, if we predict that all the leaves are in label "3", we get an accuracy of 61%.
## Handling Class Imbalance Using KFold Validation and ImageDataGenerator
def create_folds(df_train, n_split):
    df_train.loc[:, "kfold"] = -1
    df_train = df_train.sample(frac=1).reset_index(drop=True)
    SS = StratifiedKFold(n_splits=n_split)
    y = df_train.label.values
    for fold, (t_,v_) in enumerate((SS.split(X=df_train,y=y))):
        df_train.loc[v_, "kfold"] = fold
    return df_train
df_train['label'] = df_train['label'].astype('str')
df_train = create_folds(df_train, 5)
df_train.head(5)
df_train['kfold'].value_counts()
## Apply Data Transforms and Specify Data Loaders
train_dir = 'cassava-leaf-disease-classification/train_images'

def data_generator(train, validation , batch_size, img_rows, img_cols):
        
        train_datagen = ImageDataGenerator(rescale=1./225,
                           rotation_range=40,
                           width_shift_range=0.2,
                           height_shift_range=0.2,
                           shear_range=0.2,
                           zoom_range=0.2,
                           horizontal_flip=True,)
        
        validation_datagen = ImageDataGenerator(rescale=1./255,)
        
        train_gen = train_datagen.flow_from_dataframe(train,
                                                    train_dir,
                                                    x_col = 'image_id',
                                                    y_col = 'label',
                                                    target_size=(img_rows, img_cols),
                                                    seed = 42,
                                                    batch_size=batch_size,
                                                    class_mode='categorical',
                                                    interpolation = 'nearest',
                                                    shuffle=True)

        validation_gen = validation_datagen.flow_from_dataframe(validation,
                                                    train_dir,
                                                    x_col = 'image_id',
                                                    y_col = 'label',
                                                    target_size=(img_rows, img_cols),
                                                    seed = 42,
                                                    batch_size=batch_size,
                                                    class_mode='categorical',
                                                    interpolation = 'nearest',
                                                    shuffle=False)
        
        return train_gen, validation_gen
## Base Model - CNN Model
### Model Architecture
def create_model():
    
    # Instantiate model
    model = models.Sequential()
    
    # Add Convolution layer first
    model.add(layers.Conv2D(filters=32, kernel_size=(3,3), input_shape=(224, 224, 3), activation="relu"))
    model.add(layers.MaxPool2D(pool_size=(2,2)))
    
    # Add Convolution layer first
    model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation="relu"))
    model.add(layers.MaxPool2D(pool_size=(2,2)))
    
    # Add Convolution layer first
    model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation="relu"))
    model.add(layers.MaxPool2D(pool_size=(2,2)))
    
    # Add Convolution layer first
    model.add(layers.Conv2D(filters=128, kernel_size=(3,3), activation="relu"))
    model.add(layers.MaxPool2D(pool_size=(2,2)))
    
    # Flatten out the model
    model.add(layers.Flatten())
    
    # Add Dense layer now
    model.add(layers.Dense(128, activation='relu'))
    model.add(layers.Dropout(0.5))
    
    # Add output layer
    model.add(layers.Dense(5, activation='softmax'))
    
    # Compile the model
    model.compile(optimizer='adam', loss = "categorical_crossentropy", metrics = ["accuracy"]) 
    
    return model
base_model = create_model()
base_model.summary()
def train_model(df, batch_size, img_rows, img_cols, fold, model):
    
    t1_1 = datetime.now()
    
    train = df[df.kfold != fold].reset_index(drop=True)
    validation = df[df.kfold == fold].reset_index(drop=True)
    
    train_generator, validation_generator = data_generator(train, validation, batch_size, img_rows, img_cols)
    
    callback_list = [callbacks.EarlyStopping(monitor='val_accuracy', patience=3, mode='max', restore_best_weights=True),
                    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, mode='min'),]
      
    history = model.fit(train_generator, validation_data=validation_generator, callbacks=callback_list, epochs=10)
    
    return model, history, train_generator, validation_generator
base_oof_acc=[]
base_oof_val_acc=[]
base_oof_loss=[]
base_oof_val_loss=[]
base_model_time = []

for i in range(5):
    
    t1_1 = datetime.now()
    
    print(25*"-")    
    print(f'{i}-fold training')
    print(25*"-")

    base_model, history, train_generator, validation_generator = train_model(df_train, 32, 224, 224, i, base_model)

    train_accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']

    base_oof_acc.append(train_accuracy)
    base_oof_val_acc.append(val_accuracy)
    base_oof_loss.append(train_loss)
    base_oof_val_loss.append(val_loss)

    epochs = range(1, len(train_accuracy) + 1)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    fig.set_size_inches(20,10)

    ax1.plot(epochs , train_accuracy , 'go-' , label = 'Training Accuracy')
    ax1.plot(epochs , val_accuracy , 'ro-' , label = 'Validation Accuracy')
    ax1.set_title('Training & Validation Accuracy')
    ax1.legend()
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Accuracy")

    ax2.plot(epochs , train_loss , 'g-o' , label = 'Training Loss')
    ax2.plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')
    ax2.set_title('Testing Accuracy & Loss')
    ax2.legend()
    ax2.set_xlabel("Epochs")
    ax2.set_ylabel("Training & Validation Loss")

    fig.tight_layout()
    plt.show()
    
    t2_1 = datetime.now()
    base_model_time.append(str(t2_1 - t1_1))
    print("Model computation time: " + str(t2_1 - t1_1))
base_acc_average = [sum(sub_list) / len(sub_list) for sub_list in base_oof_acc]
base_val_acc_average = [sum(sub_list) / len(sub_list) for sub_list in base_oof_val_acc]
base_loss_average = [sum(sub_list) / len(sub_list) for sub_list in base_oof_loss]
base_val_loss_average = [sum(sub_list) / len(sub_list) for sub_list in base_oof_val_loss]
print("Average training accuracy across 5 folds with ten epochs each -", base_acc_average)
print("Average validation accuracy across 5 folds with ten epochs each -", base_val_acc_average)
print("Average training loss across 5 folds with ten epochs each -", base_loss_average)
print("Average validation loss across 5 folds with ten epochs each -", base_val_loss_average)
base_model_time
base_acc_average = [sum(sub_list) / len(sub_list) 
                    for sub_list in base_oof_acc]
base_acc_average
print("KFold Validation Accuracy: ", np.hstack(base_oof_acc).mean())
Y_pred = base_model.predict_generator(validation_generator)
y_pred = np.argmax(Y_pred, axis=1)
print('Classification Report')
target_names = list(train_generator.class_indices.keys()) # Classes
print(classification_report(validation_generator.classes, y_pred, target_names=target_names))
cm = confusion_matrix(validation_generator.classes, y_pred)
labels = ['Cassava Bacterial Blight (CBB)', 'Cassava Brown Streak Disease (CBSD)', 'Cassava Green Mottle (CGM)', 'Cassava Mosaic Disease (CMD)','Healthy']
plt.figure(figsize=(8,6))
sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap="Blues", vmin = 0.2);
plt.title('Confusion Matrix')
plt.ylabel('True Class')
plt.xlabel('Predicted Class')
plt.show()
pred = pd.DataFrame(y_pred, columns=['class_pred'])
pred['pred'] = pred['class_pred'].map({int(i) : c for i, c in disease_class.items()})
validation = df_train[df_train.kfold == 4].reset_index(drop=True)

image_ids_test = validation["image_id"].values
labels_test = validation["class"].values
pred_result = pred['pred'].values
rand_idxs = np.random.permutation(len(validation))[:9]
plot(image_ids_test[rand_idxs], labels_test[rand_idxs], pred_result[rand_idxs])
plt.show()
## Transfer Learning
Here pretrained models will be used to train the dataset. Three models will be tested and compared against each other - namely VGG 16, ResNet50 and EfficientNetB3
### VGG-16 Model Architecture
def create_model():

    # import model
    vgg16 = applications.VGG16(include_top = False, weights = 'imagenet',
                                                input_tensor = keras.Input(shape=(224, 224, 3)))
    # freeze some convolution blocks
    for layer in vgg16.layers[:-3]: 
        layer.trainable = False
    
    # define inputs
    inputs = layers.Input(shape=(224, 224, 3))
    
    # call the base model on inputs
    x = vgg16(inputs)


    # Fine-tuning
      # Combos
    x = layers.GlobalAveragePooling2D()(x) #add global average pooling layer 
    x = layers.Dense(128, activation='relu')(x) 
    x = layers.Dropout(rate = 0.5)(x)
    x = layers.ReLU()(x)
   
    # Output layer - output dimension=5 because we have 5 classes
    outputs = layers.Dense(5, activation = "softmax")(x)

    # Put the model together
    model = keras.Model(inputs=inputs, outputs=outputs)
    
    # compile model with Adam, categorical_crossentropy loss, and accuracy metric
    model.compile(optimizer="adam", loss='categorical_crossentropy', metrics=['accuracy'])
 
    return model
vgg16_model = create_model()
vgg16_model.summary()
def train_model(df, batch_size, img_rows, img_cols, fold, model):
    
    train = df[df.kfold != fold].reset_index(drop=True)
    validation = df[df.kfold == fold].reset_index(drop=True)
    
    train_generator, validation_generator = data_generator(train, validation, batch_size, img_rows, img_cols)
    
    callback_list = [callbacks.EarlyStopping(monitor='val_accuracy',patience=3, mode='max', restore_best_weights=True),
                    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, mode='min'),]
    
    
    history = model.fit(train_generator, validation_data=validation_generator, callbacks=callback_list, epochs=10)
    
    return model, history, train_generator, validation_generator
vgg_oof_acc=[]
vgg_oof_val_acc=[]
vgg_oof_loss=[]
vgg_oof_val_loss=[]
vgg_model_time = []

for i in range(5):
    
    t1_1 = datetime.now()
    
    print(25*"-")    
    print(f'{i}-fold training')
    print(25*"-")

    vgg_model, history, train_generator, validation_generator = train_model(df_train, 32, 224, 224, i, vgg16_model)

    train_accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']

    vgg_oof_acc.append(train_accuracy)
    vgg_oof_val_acc.append(val_accuracy)
    vgg_oof_loss.append(train_loss)
    vgg_oof_val_loss.append(val_loss)

    epochs = range(1, len(train_accuracy) + 1)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    fig.set_size_inches(20,10)

    ax1.plot(epochs , train_accuracy , 'go-' , label = 'Training Accuracy')
    ax1.plot(epochs , val_accuracy , 'ro-' , label = 'Validation Accuracy')
    ax1.set_title('Training & Validation Accuracy')
    ax1.legend()
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Accuracy")

    ax2.plot(epochs , train_loss , 'g-o' , label = 'Training Loss')
    ax2.plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')
    ax2.set_title('Testing Accuracy & Loss')
    ax2.legend()
    ax2.set_xlabel("Epochs")
    ax2.set_ylabel("Training & Validation Loss")

    fig.tight_layout()
    plt.show()
    
    t2_1 = datetime.now()
    vgg_model_time.append(str(t2_1 - t1_1))
    print("Model computation time: " + str(t2_1 - t1_1))
vgg_acc_average = [sum(sub_list) / len(sub_list) for sub_list in vgg_oof_acc]
vgg_val_acc_average = [sum(sub_list) / len(sub_list) for sub_list in vgg_oof_val_acc]
vgg_loss_average = [sum(sub_list) / len(sub_list) for sub_list in vgg_oof_loss]
vgg_val_loss_average = [sum(sub_list) / len(sub_list) for sub_list in vgg_oof_val_loss]
print("Average training accuracy across 5 folds with ten epochs each", vgg_acc_average)
print("Average validation accuracy across 5 folds with ten epochs each", vgg_val_acc_average)
print("Average training loss across 5 folds with ten epochs each", vgg_loss_average)
print("Average validation loss across 5 folds with ten epochs each", vgg_val_loss_average)
vgg_model_time
print("KFold Validation Accuracy: ", np.hstack(vgg_oof_acc).mean())
Y_pred = vgg_model.predict_generator(validation_generator)
y_pred = np.argmax(Y_pred, axis=1)
print('Classification Report')
target_names = list(train_generator.class_indices.keys()) # Classes
print(classification_report(validation_generator.classes, y_pred, target_names=target_names))
cm = confusion_matrix(validation_generator.classes, y_pred)
labels = ['Cassava Bacterial Blight (CBB)', 'Cassava Brown Streak Disease (CBSD)', 'Cassava Green Mottle (CGM)', 'Cassava Mosaic Disease (CMD)','Healthy']
plt.figure(figsize=(8,6))
sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap="Blues", vmin = 0.2);
plt.title('Confusion Matrix')
plt.ylabel('True Class')
plt.xlabel('Predicted Class')
plt.show()
pred = pd.DataFrame(y_pred, columns=['class_pred'])
pred['pred'] = pred['class_pred'].map({int(i) : c for i, c in disease_class.items()})
validation = df_train[df_train.kfold == 4].reset_index(drop=True)

image_ids_test = validation["image_id"].values
labels_test = validation["class"].values
pred_result = pred['pred'].values
rand_idxs = np.random.permutation(len(validation))[:9]
plot(image_ids_test[rand_idxs], labels_test[rand_idxs], pred_result[rand_idxs])
plt.show()
#### ResNet Model Architecture
def create_model():

    # import model
    restnet101 = applications.ResNet101V2(include_top = False, weights = 'imagenet', 
                                                input_tensor = keras.Input(shape=(224, 224, 3)))
    # freeze some convolution blocks
    for layer in restnet101.layers[:-13]: 
        layer.trainable = False
    # check frozen layers
#     for i, layer in enumerate(restnet101.layers):
#         print(i, layer.name, layer.trainable)
    
    # define inputs
    inputs = layers.Input(shape=(224, 224, 3))
    # call the base model on x
    x = restnet101(inputs)


    # Fine-tuning
      # Combos
    x = layers.GlobalAveragePooling2D(name="avg_pool")(x)
    x = layers.Dense(256, activation = "relu")(x)
    x = layers.Dense(256, activation = "relu")(x)
    x = layers.Dropout(0.2)(x)
    x = layers.ReLU()(x)
    # Output layer - output dimension=5 because we have 5 classes
    outputs = layers.Dense(5, activation="softmax", name="pred")(x)

    # Put the model together
    model = keras.Model(inputs, outputs, name="EfficientNet")
    # Compile
    model.compile(optimizer='adam', loss = "categorical_crossentropy", metrics = ["accuracy"]) 
 
    return model
     
resnet_model = create_model()
resnet_model.summary()
def train_model(df, batch_size, img_rows, img_cols, fold, model):
    
    t1_1 = datetime.now()
    
    train = df[df.kfold != fold].reset_index(drop=True)
    validation = df[df.kfold == fold].reset_index(drop=True)
    
    train_generator, validation_generator = data_generator(train, validation, batch_size, img_rows, img_cols)
    
    callback_list = [callbacks.EarlyStopping(monitor='val_accuracy',patience=3, mode='max', restore_best_weights=True),
                    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, mode='min'),]
      
    history = model.fit(train_generator, validation_data=validation_generator, callbacks=callback_list, epochs=10)
    
    return model, history, train_generator, validation_generator
resnet_oof_acc=[]
resnet_oof_val_acc=[]
resnet_oof_loss=[]
resnet_oof_val_loss=[]
resnet_model_time = []

for i in range(5):
    
    t1_1 = datetime.now()
    
    print(25*"-")    
    print(f'{i}-fold training')
    print(25*"-")

    resnet_model, history, train_generator, validation_generator = train_model(df_train, 32, 224, 224, i, resnet_model)

    train_accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']

    resnet_oof_acc.append(train_accuracy)
    resnet_oof_val_acc.append(val_accuracy)
    resnet_oof_loss.append(train_loss)
    resnet_oof_val_loss.append(val_loss)


    epochs = range(1, len(train_accuracy) + 1)
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))
    fig.set_size_inches(20,10)

    ax1.plot(epochs , train_accuracy , 'go-' , label = 'Training Accuracy')
    ax1.plot(epochs , val_accuracy , 'ro-' , label = 'Validation Accuracy')
    ax1.set_title('Training & Validation Accuracy')
    ax1.legend()
    ax1.set_xlabel("Epochs")
    ax1.set_ylabel("Accuracy")

    ax2.plot(epochs , train_loss , 'g-o' , label = 'Training Loss')
    ax2.plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')
    ax2.set_title('Testing Accuracy & Loss')
    ax2.legend()
    ax2.set_xlabel("Epochs")
    ax2.set_ylabel("Training & Validation Loss")

    fig.tight_layout()
    plt.show()
    
    t2_1 = datetime.now()
    resnet_model_time.append(str(t2_1 - t1_1))
    print("Model computation time: " + str(t2_1 - t1_1))
resnet_acc_average = [sum(sub_list) / len(sub_list) for sub_list in resnet_oof_acc]
resnet_val_acc_average = [sum(sub_list) / len(sub_list) for sub_list in resnet_oof_val_acc]
resnet_loss_average = [sum(sub_list) / len(sub_list) for sub_list in resnet_oof_loss]
resnet_val_loss_average = [sum(sub_list) / len(sub_list) for sub_list in resnet_oof_val_loss]
print("Average training accuracy across 5 folds with ten epochs each -", resnet_acc_average)
print("Average validation accuracy across 5 folds with ten epochs each -", resnet_val_acc_average)
print("Average training loss across 5 folds with ten epochs each -", resnet_loss_average)
print("Average validation loss across 5 folds with ten epochs each -", resnet_val_loss_average)
resnet_model_time
print("KFold Validation Accuracy: ", np.hstack(resnet_oof_acc).mean())
Y_pred = resnet_model.predict_generator(validation_generator)
y_pred = np.argmax(Y_pred, axis=1)
print('Classification Report')
target_names = list(train_generator.class_indices.keys()) # Classes
print(classification_report(validation_generator.classes, y_pred, target_names=target_names))
cm = confusion_matrix(validation_generator.classes, y_pred)
labels = ['Cassava Bacterial Blight (CBB)', 'Cassava Brown Streak Disease (CBSD)', 'Cassava Green Mottle (CGM)', 'Cassava Mosaic Disease (CMD)','Healthy']
plt.figure(figsize=(8,6))
sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d', cmap="Blues", vmin = 0.2);
plt.title('Confusion Matrix')
plt.ylabel('True Class')
plt.xlabel('Predicted Class')
plt.show()
pred = pd.DataFrame(y_pred, columns=['class_pred'])
pred['pred'] = pred['class_pred'].map({int(i) : c for i, c in disease_class.items()})
validation = df_train[df_train.kfold == 4].reset_index(drop=True)

image_ids_test = validation["image_id"].values
labels_test = validation["class"].values
pred_result = pred['pred'].values
rand_idxs = np.random.permutation(len(validation))[:9]
plot(image_ids_test[rand_idxs], labels_test[rand_idxs], pred_result[rand_idxs])
plt.show()
Of the three models (Base CNN, VGG16, ResNet101v2), ResNet101v2 shows the better performance overall.
### Model Comparision
# Training Accuracy
df_train_accuracy = pd.DataFrame(
    {'Fold': ['1', '2', '3', '4', '5'],
     'Base CNN': base_acc_average,
     'VGG16': vgg_acc_average,
     'ResNet101v2': resnet_acc_average
    })

df_train_accuracy
# convert to long (tidy) form
df_train_accuracy_T = df_train_accuracy.melt('Fold', var_name='Model', value_name='Training Accuracy')
df_train_accuracy_T
# plt.figure(figsize=(6,8))

sns.catplot(x="Fold", y="Training Accuracy", hue='Model', data=df_train_accuracy_T, kind='point')
plt.title("Training Accuracy Across 5 Folds")
plt.show()
# Validation Accuracy
df_val_accuracy = pd.DataFrame(
    {'Fold': ['1', '2', '3', '4', '5'], 
     'Base CNN': base_val_acc_average,
     'VGG16': vgg_val_acc_average,
     'ResNet101v2': resnet_val_acc_average
    })

df_val_accuracy
# convert to long (tidy) form
df_val_accuracy_T = df_val_accuracy.melt('Fold', var_name='Model', value_name='Validation Accuracy')
df_val_accuracy_T
sns.catplot(x="Fold", y="Validation Accuracy", hue='Model', data=df_val_accuracy_T, kind='point')
plt.title("Validation Accuracy Across 5 Folds")
plt.show()
# Train Loss
df_train_loss = pd.DataFrame(
    {'Fold': ['1', '2', '3', '4', '5'], 
     'Base CNN': base_loss_average,
     'VGG16': vgg_loss_average,
     'ResNet101v2': resnet_loss_average
    })

df_train_loss
df_train_loss_T = df_train_loss.melt('Fold', var_name='Model', value_name='Train Loss')
df_train_loss_T
sns.catplot(x="Fold", y="Train Loss", hue='Model', data=df_train_loss_T, kind='point')
plt.title("Training Loss Across 5 Folds")
plt.show()
# Validation Loss
df_val_loss = pd.DataFrame(
    {'Fold': ['1', '2', '3', '4', '5'], 
     'Base CNN': base_val_loss_average,
     'VGG16': vgg_val_loss_average,
     'ResNet101v2': resnet_val_loss_average
    })

df_val_loss
df_val_loss_T = df_val_loss.melt('Fold', var_name='Model', value_name='Validation Loss')
df_val_loss_T
sns.catplot(x="Fold", y="Validation Loss", hue='Model', data=df_val_loss_T, kind='point')
plt.title("Validation Loss Across 5 Folds")
plt.show()
